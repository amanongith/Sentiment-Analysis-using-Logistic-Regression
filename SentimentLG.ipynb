{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1NMCXpErwIzm"},"outputs":[],"source":["#importing necessary libs\n","\n","import nltk, re, string\n","from nltk.corpus import stopwords, twitter_samples\n","import numpy as np\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tl_tQx0ZCs4e"},"outputs":[],"source":["#Preprocessing of the tweets that is our data\n","\n","def process_tweet(tweet):\n","  stemmer = nltk.PorterStemmer();\n","  stopwords_english = stopwords.words('english')\n","  tweet = re.sub(r'\\$\\w*', '', tweet)\n","  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","  tweet = re.sub(r'#', '', tweet)\n","  tokenizer = nltk.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n","  tweet_tokens = tokenizer.tokenize(tweet)\n","\n","  tweets_clean = []\n","  for word in tweet_tokens:\n","    if (word not in stopwords_english and\n","        word not in string.punctuation):\n","      stem_word = stemmer.stem(word) \n","      tweets_clean.append(stem_word)\n","  return tweets_clean\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ut7gEKQyGtN-"},"outputs":[],"source":["#build_freqs func\n","\n","def build_freqs(tweets,ys):\n","  \"\"\"Build frequencies.\n","  Input:\n","    tweets: a list of tweets\n","      ys: an m x 1 array with sentiment label of each tweet\n","           (0 or 1)\n","  Output:\n","    freqs: a dictionary mapping each (word, sentiment) pair to its frequency\n","  \"\"\"\n","  yslist = np.squeeze(ys).tolist()\n","\n","  freqs = {}\n","  for y, tweet in zip(yslist, tweets):\n","    for word in process_tweet(tweet):\n","      pair = (word, y)\n","      if pair in freqs:\n","        freqs[pair] += 1\n","      else:\n","        freqs[pair] = 1\n","  return freqs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2029,"status":"ok","timestamp":1685559666246,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"yZ-9QddEKNwf","outputId":"122c50f3-d7b0-4baa-d4a2-56d566869c38"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"]},{"name":"stdout","output_type":"stream","text":["{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('twitter_samples')\n","nltk.download('stopwords')\n","tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n","ys = [1, 0, 0, 0, 0]\n","res = build_freqs(tweets, ys)\n","print(res)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2551,"status":"ok","timestamp":1685560293941,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"wZQgXp9bLioD","outputId":"0853230d-4a85-44ed-9872-ebe2f8f70a76"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Package twitter_samples is already up-to-date!\n"]}],"source":["#select the set of positive and negative tweets\n","nltk.download('twitter_samples')\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":430,"status":"ok","timestamp":1685560513039,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"s6tQ49XaOfd6"},"outputs":[],"source":["#splittling data into two pieces (for training and testing)\n","test_pos = all_positive_tweets[4000:]\n","train_pos = all_positive_tweets[:4000]\n","test_neg = all_negative_tweets[4000:]\n","train_neg = all_negative_tweets[:4000]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":702,"status":"ok","timestamp":1685560557308,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"bdBX-fyBPGK8"},"outputs":[],"source":["train_x = train_pos + train_neg\n","test_x = test_pos + test_neg"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":462,"status":"ok","timestamp":1685560955087,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"ANefjd_pPRgq"},"outputs":[],"source":["#combining positive and negative labels(building y - target var)\n","train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n","test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4094,"status":"ok","timestamp":1685561018305,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"ol_cLEKvQxky"},"outputs":[],"source":["#create frequency dictionary\n","freqs = build_freqs(train_x, train_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":478,"status":"ok","timestamp":1685561168937,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"O6yaxoTVRBny","outputId":"8580c0a6-1a87-4ad6-dbf4-2c4a5f1a2d17"},"outputs":[{"name":"stdout","output_type":"stream","text":["type(freqs) = <class 'dict'>\n","len(freqs) = 11337\n"]}],"source":["#checking outputs\n","\n","print(\"type(freqs) = \" + str(type(freqs)))\n","print(\"len(freqs) = \" + str(len(freqs.keys())))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":476,"status":"ok","timestamp":1685561419820,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"Y4GjQH6BR1_6","outputId":"2e91a229-94bc-4477-9b6f-3c98ae924eb8"},"outputs":[{"name":"stdout","output_type":"stream","text":["this is an example of positive tweet: \n"," @gculloty87 Yeah I suppose she was lol! Chat in a bit just off out x :))\n","\n"," this is an example of the processed version of the tweet: \n"," ['yeah', 'suppos', 'lol', 'chat', 'bit', 'x', ':)']\n"]}],"source":["#test the function\n","\n","print('this is an example of positive tweet: \\n', train_x[22])\n","print('\\n this is an example of the processed version of the tweet: \\n', process_tweet(train_x[22]))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1685561823908,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"PHNMeiQPTPRn"},"outputs":[],"source":["#logistic regression\n","#sigmoid func\n","\n","def sigmoid(z):\n","  \"\"\"\n","  Input:\n","    z: is the input (can be scalar or an array)\n","  Output:\n","    h: the sigmoid of z\n","  \"\"\"\n","  zz = np.negative(z)\n","  h = 1 / (1 + np.exp(zz))\n","  return h"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":505,"status":"ok","timestamp":1685562903692,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"Yib4u_SdUGa4"},"outputs":[],"source":["#cost func and gradient\n","\n","def gradientDiscent(x, y, theta, alpha, num_iters):\n","  \"\"\"\n","  Input:\n","    x: matrix of features which is (m,n+1)\n","    y: corresponding labels of the input matrix x, dimentions (m,1)\n","    theta: weight vector of dimension (n+1,1)\n","    alpha: learning rate\n","    num_iters: number of iterations to train the model\n","  Output:\n","    J: the final cost\n","  \"\"\"\n","\n","  m = x.shape[0]\n","  for i in range(0, num_iters):\n","    z = np.dot(x, theta)\n","    h = sigmoid(z)\n","    cost = -1. / m * (np.dot(y.transpose(), np.log(h)) + np.dot((1 - y).transpose(), np.log(1 - h)))\n","    theta = theta - (alpha / m) * np.dot(x.transpose(), (h - y))\n","  cost = float(cost)\n","  return cost, theta\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":481,"status":"ok","timestamp":1685563525157,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"k-3xtdFuYs07"},"outputs":[],"source":["#extracting features\n","def extract_features(tweet, freqs):\n","  \"\"\"\n","  Input:\n","    tweet: a label of words for one tweet\n","    freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","  Output:\n","    x: a feature vector of dimension (1,3)\n","  \"\"\"\n","\n","  word_1 = process_tweet(tweet)\n","  x = np.zeros((1, 3))\n","  \n","  #bias term is set to 1\n","  x[0, 0] = 1\n","\n","  for word in word_1:\n","    #incrementing the word count for positive label 1\n","    x[0, 1] += freqs.get((word, 1.0), 0)\n","    #incrementing the word count for negative label 0\n","    x[0, 2] += freqs.get((word, 0.0), 0)\n","\n","  assert (x.shape == (1, 3))\n","  return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":492,"status":"ok","timestamp":1685563625145,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"EevxqTo8anPU","outputId":"50cf0ad0-aaac-42f9-a25f-47a9002f0519"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.000e+00 3.006e+03 1.240e+02]]\n"]}],"source":["#test on training data\n","tmp1 = extract_features(train_x[22], freqs)\n","print(tmp1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6153,"status":"ok","timestamp":1685564105691,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"QQvlgJ5ibDl0"},"outputs":[],"source":["#training the model\n","\n","#collect the features 'x' and stack them into a matrix 'x'\n","X = np.zeros((len(train_x), 3))\n","for i in range(len(train_x)):\n","  X[i, :] = extract_features(train_x[i], freqs)\n","\n","#traing labels corresponding to X\n","Y = train_y\n","\n","#applying gradient descent\n","#these values are predefined(Andrew NG)\n","J, theta = gradientDiscent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":816,"status":"ok","timestamp":1685564359876,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"VDrAAiVYczRg"},"outputs":[],"source":["def predict_tweet(tweet, freqs, theta):\n","  \"\"\"\n","  Input:\n","    tweet: a string\n","    freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","    theta: (3,1) vector of weights\n","  Output:\n","    y_pred: the probability of a tweet being positive or negative\n","  \"\"\"\n","  #extract_features of tweet and store into x\n","  x = extract_features(tweet, freqs)\n","  y_pred = sigmoid(np.dot(x, theta))\n","\n","  return y_pred\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":471,"status":"ok","timestamp":1685565050103,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"rl0zit2ndxwr"},"outputs":[],"source":["def test_logistic_regression(test_x, test_y, freqs, theta):\n","  \"\"\"\n","  Input:\n","    test_x: a list of tweets\n","    test_y: (m, 1) vector with the corresponding labels for the list of tweets\n","    freqs: a dictionary with the frequency of each pair\n","    theta: weight vector of dimension (3, 1)\n","  Output:\n","    accuracy: (# of tweets classified correctly) / (total # of tweets)\n","  \"\"\"\n","  #list for storing prediction\n","  y_hat = []\n","\n","  for tweet in test_x:\n","    #get the label prediction for the tweet\n","    y_pred = predict_tweet(tweet, freqs, theta)\n","    if y_pred > 0.5:\n","      y_hat.append(1)\n","    else:\n","      y_hat.append(0)\n","    \n","  accuracy = (y_hat == np.squeeze(test_y)).sum() / len(test_x)\n","  return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1286,"status":"ok","timestamp":1685565190061,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"1v6XZ_9kgZ9r","outputId":"b5fe3ed4-f1c5-44a7-8f17-9aa4198353a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic regression model's accuracy =0.9950\n"]}],"source":["tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n","print(f\"Logistic regression model's accuracy ={tmp_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":470,"status":"ok","timestamp":1685565384082,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"phH8K_4Dg84u"},"outputs":[],"source":["#predicting with own tweet\n","\n","def pre(sentence):\n","  yhat = predict_tweet(sentence, freqs, theta)\n","  if yhat > 0.5:\n","    return 'Positive sentiment'\n","  elif yhat == 0:\n","    return 'Neutral sentiment'\n","  else:\n","    return 'Negative sentiment'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":673,"status":"ok","timestamp":1685565512121,"user":{"displayName":"Aman Shaw","userId":"02188556081047904070"},"user_tz":-330},"id":"tjwNYlb7hrN6","outputId":"c0b45290-ba9b-40b5-a9be-e03d9918564b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive sentiment\n"]}],"source":["my_tweet = 'it is very bad habit'\n","res = pre(my_tweet)\n","print(res)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOcXa6bKVqex6+eSvX643oh","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
